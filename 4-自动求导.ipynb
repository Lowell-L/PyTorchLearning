{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 自动求导 autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.autograd.backward()\n",
    "函数原型：torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)\n",
    "<br>功能：自动求取梯度\n",
    "<br>tensors: 用于求导的张量，如 loss\n",
    "<br>retain_graph: 保存计算图。PyTorch 采用动态图机制，**默认每次反向传播之后都会释放计算图。这里设置为 True 可以不释放计算图。**\n",
    "<br>create_graph: 创建导数计算图，用于高阶求导。\n",
    "<br>grad_tensors: 多梯度权重。当有多个 loss 混合需要计算梯度时，设置每个 loss 的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解释 retain_graph\n",
    "在 tensor.backward(retain_graph=True) 的情况下，**第一次求导后保存计算图，可以再次进行求导。否则计算图会被释放，无法再次求导。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient: tensor([5.], dtype=torch.float64)\n",
      "gradient: tensor([10.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor([1], requires_grad=True, dtype=float)\n",
    "x = torch.tensor([2], requires_grad=True, dtype=float)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "# y.backward()\n",
    "# print(\"gradient:\", w.grad)\n",
    "# y.backward()      # RuntimeError: 试图对计算图第二次调用 backward()，当调用.backward()或autograd.grad()时，计算图中保存的导数中间值将被释放。\n",
    "                    # 如果需要对计算图二次求导，指定 retain_graph=True\n",
    "y.backward(retain_graph=True)\n",
    "print(\"First gradient:\", w.grad)\n",
    "y.backward()\n",
    "print(\"Second gradient:\", w.grad)\n",
    "# y.backward()      # 要想下一次计算图可以再次自动求导，那么这次求导时参数就需要指定 retain_graph=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解释 grad_tensors\n",
    "多个损失函数，多权重求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1], requires_grad=True, dtype=float)\n",
    "x = torch.tensor([2], requires_grad=True, dtype=float)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "\n",
    "y0 = torch.mul(a, b)\n",
    "y1 = torch.add(a, b)\n",
    "\n",
    "loss = torch.cat((y0, y1), dim=0)   # 将两个函数拼接在一起 2*1维\n",
    "grad_tensor = torch.tensor([1, 2])     # 设置两个 loss 的权重: y0 的权重是 1，y1 的权重是 2\n",
    "\n",
    "loss.backward(gradient=grad_tensor)    # 最终的 w 的导数由两部分组成。∂y0/∂w * 1 + ∂y1/∂w * 2\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该 loss 由两部分组成：$y{0}$ 和 $y{1}$。其中 $\\frac{\\partial y{0}}{\\partial w}=5$，$\\frac{\\partial y{1}}{\\partial w}=2$。而 gradtensor 设置两个 loss 对 w 的权重分别为 1 和 2。因此最终 w 的梯度为：$\\frac{\\partial y_{0}}{\\partial w} \\times 1+ \\frac{\\partial y_{1}}{\\partial w} \\times 2=9$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.autograd.grad()\n",
    "函数原型：torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\n",
    "<br>功能：求取梯度\n",
    "<br>outputs: 用于求导的张量，如 loss\n",
    "<br>inputs: 需要梯度的张量，如 w\n",
    "<br>create_graph: 创建导数计算图，用于高阶求导。\n",
    "<br>retain_graph:保存计算图，可以再次求导\n",
    "<br>grad_tensors: 多梯度权重。当有多个 loss 混合需要计算梯度时，设置每个 loss 的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4.], dtype=torch.float64, grad_fn=<MulBackward0>),)\n",
      "(tensor([2.], dtype=torch.float64),)\n"
     ]
    }
   ],
   "source": [
    "# 计算高阶导\n",
    "x = torch.tensor([2], dtype=float, requires_grad=True)\n",
    "y = torch.pow(x, 2)     # y = x**2\n",
    "gradFirst = torch.autograd.grad(y, x, create_graph=True)    # 求一阶导，指定参数 create_graph=True 是为了让一阶导数 grad_1 也拥有计算图，利用此计算图求二阶导\n",
    "print(gradFirst)                                            # 返回的是一个元组，第 0 个元素才是真正的梯度\n",
    "gradSecond = torch.autograd.grad(gradFirst[0], x)\n",
    "print(gradSecond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **在每次反向传播求导时，计算的梯度不会自动清零。如果进行多次迭代计算梯度而没有清零，那么梯度会在前一次的基础上叠加。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不清零状态：\n",
      "tensor([6.])\n",
      "tensor([12.])\n",
      "tensor([18.])\n",
      "tensor([24.])\n",
      "清零状态：\n",
      "tensor([6.])\n",
      "tensor([6.])\n",
      "tensor([6.])\n",
      "tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([2.], requires_grad=True)\n",
    "x = torch.tensor([1.], requires_grad=True)\n",
    "print(\"不清零状态：\")\n",
    "for i in range(0, 4):\n",
    "    a = torch.add(w, x)\n",
    "    b = torch.add(w, 1)\n",
    "    y = torch.mul(a, b)\n",
    "    y.backward()\n",
    "    print(w.grad)\n",
    "\n",
    "# 使用w.grad.zero_()将梯度清零。\n",
    "print(\"清零状态：\")\n",
    "for i in range(0, 4):\n",
    "    a = torch.add(w, x)\n",
    "    b = torch.add(w, 1)\n",
    "    y = torch.mul(a, b)\n",
    "    w.grad.zero_()      # 清零\n",
    "    y.backward()\n",
    "    print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inplace操作 未读 TODO！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
